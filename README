Dear reader,

this is the source code of NOLAM.

# Generated traces
The set of traces is contained into the folder "Analysis/Input traces/NOLAM", the traces are grouped by domain.
Each domain folder contains a directory for every noise level in [0,1]. For example, the set of 10
noisy traces (with noise level 0.2) used for learning an action model for the domain "blocksworld" is store into
the directory "Analysis/Input trace/NOLAM/blocksworld/noisy_states/0.2".
The problems used for generating the traces of each domain "x" are contained into the directory "Analysis/Benchmarks/x"

# Running NOLAM
To run NOLAM on all domains in "Analysis/Input trace/NOLAM/", and every noise level in [0, 1], run the main.py script.
The results of each i-th run are stored into a directory in "Analysis/Results/NOLAM/noisy_states/runi".
For example, when running NOLAM for the first time stores the results into the directory
"Analysis/Results/NOLAM/noisy_states/run0".
In the run directory, you can find one directory for each considered domain (e.g. "Analysis/Results/NOLAM/noisy_states/run0").
Each domain directory contain one subdirectory for each considered level of noise (e.g. "Analysis/Results/NOLAM/noisy_states/run0/0.2"),
where you can find three files:
1. log: contains some debugging information
2. model.pddl: the learned model
3.op_stats.json: for each operator "op" and potential precondition/effect of the operator, i.e. P(param(op)), we store
the count of transitions where the ground atom is true/false after/before executing an instantiation of the operator.

Finally, each run directory contains the detailed results over all domains in a pandas dataframe,
which is named 'nolam_results.xlsx'.

# Paper results
This code can be used for reproducing the paper results reported in Table 3 and Figures 3 and 4, we report all the
models, logs and dataframes in the directory "Analysis/ICAPS/Results". We also report the dataframes with the results
of planminer and alice.
The results tables and figures in the paper can be generated by means of the "Util/compare.py" script, the ground
truth model are contained in the directory "Analysis/Benchmarks".
For reproducing the results for the plans validation (i.e. Figures 5 and 6 in the paper), please see
the source code ActionModelValidator.